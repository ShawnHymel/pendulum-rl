{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4561acc1-bea2-4e8b-8342-e1de5391f578",
   "metadata": {},
   "source": [
    "TODO:\n",
    " * Stepper motor skipping steps--more current (better power supply)?\n",
    " * Way to pause and resume (pendulum fell off!)\n",
    " * Set up experiments (WandB?)\n",
    " * Train!\n",
    " * Normalize action and observation spaces (see: https://ai.stackexchange.com/questions/21477/why-do-we-also-need-to-normalize-the-actions-values-on-continuous-action-spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37363014-d5bd-4d0b-8093-ad182644070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install gymnasium==0.28.1\n",
    "# !python -m pip install stable-baselines3[extra]==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6601a351-afc4-4ad6-8119-335dce1392cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym version: 0.28.1\n",
      "sb3 version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from control_comms import ControlComms, StatusCode, DebugLevel\n",
    "\n",
    "# Reinforcement model modules\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import KVWriter, Logger\n",
    "\n",
    "# Check versions\n",
    "print(f\"gym version: {gym.__version__}\")\n",
    "print(f\"sb3 version: {sb3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5512c5e3-f346-4866-be07-c9f275fc1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication settings\n",
    "SERIAL_PORT = \"COM6\"    # Check your devices\n",
    "BAUD_RATE = 500000      # Must match what's in the Arduino code!\n",
    "CTRL_TIMEOUT = 1.0      # Seconds\n",
    "DEBUG_LEVEL = DebugLevel.DEBUG_ERROR\n",
    "\n",
    "# Reinforcement learning settings\n",
    "K_T = 1                 # Reward constant to multiply theta (angle of encoder)\n",
    "K_DT = 0.1              # Reward constant to multiply dtheto/dt (angular velocity of encoder)\n",
    "K_P = 0.01              # Reward constant to multiply phi (angle of stepper)\n",
    "K_DP = 0.001            # Reward constant to multiply dphi/dt (angular velocity of stepper)\n",
    "REWARD_OOB = -10_000    # Reward (penalty) for having the stepper motor move out of bounds (OOB)\n",
    "ENC_OFFSET = 180.0      # Pendulum in the \"up\" position should be 0 deg\n",
    "STP_MOVE_MIN = -10.0\n",
    "STP_MOVE_MAX = 10.0\n",
    "STP_ANGLE_MIN = -180.0  # Episode ends if stepper goes beyond this angle\n",
    "STP_ANGLE_MAX = 180.0   # Episode ends if stepper goes beyond this angle\n",
    "ENV_TIMEOUT = 10.0\n",
    "RESET_SETTLE_TIME = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631b6b53-ca9c-41c4-858c-581fcfbbe524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication constants\n",
    "CMD_SET_HOME = 0        # Set current stepper position as home (0 deg)\n",
    "CMD_MOVE_TO = 1         # Move stepper to a particular position (deg)\n",
    "CMD_MOVE_BY = 2         # Move stepper by a given amount (deg)\n",
    "CMD_SET_STEP_MODE = 3   # Set step mode\n",
    "CMD_SET_BLOCK_MODE = 4  # Set blocking mode\n",
    "CMD_NOP = 5             # Take no action, just receive observation\n",
    "STEP_MODE_1 = 0         # 1 division per step\n",
    "STEP_MODE_2 = 1         # 2 divisions per step\n",
    "STEP_MODE_4 = 2         # 4 divisions per step\n",
    "STEP_MODE_8 = 3         # 8 divisions per step\n",
    "STEP_MODE_16 = 4        # 16 divisions per step\n",
    "STATUS_OK = 0           # Stepper idle\n",
    "STATUS_STP_MOVING = 1   # Stepper is currently moving\n",
    "\n",
    "# Set to desired step mode\n",
    "STEP_MODE = STEP_MODE_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316bc585-a495-4ad3-ae94-6b0930a38bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection to Arduino board (if open)\n",
    "try:\n",
    "    controller.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6f5fef-12a0-405f-972a-fc6eb40b2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Arduino board\n",
    "controller = ControlComms(timeout=CTRL_TIMEOUT, debug_level=DEBUG_LEVEL)\n",
    "ret = controller.connect(SERIAL_PORT, BAUD_RATE)\n",
    "if ret is not StatusCode.OK:\n",
    "    print(\"ERROR: Could not connect to board\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f80790-322b-498b-bef8-211d8a2696d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 13854, False, [0.0, 0.22])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test basic comms\n",
    "controller.step(CMD_SET_STEP_MODE, [STEP_MODE_8])\n",
    "controller.step(CMD_SET_HOME, [0])\n",
    "controller.step(CMD_SET_BLOCK_MODE, [1])\n",
    "controller.step(CMD_MOVE_BY, [90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c07c94e-8ab6-490f-b856-ca817352049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close comms\n",
    "controller.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cd180-65fe-40f2-88bf-e584bdbbd2cd",
   "metadata": {},
   "source": [
    "## Build gym Environment\n",
    "\n",
    "Subclass gymnasium.Env to create a custom environment. Learn more here:<br>\n",
    "https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bcddaa-b899-4cab-9a7b-82bd214c3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulum(gym.Env):\n",
    "    \"\"\"\n",
    "    Subclass gymnasium Env class\n",
    "    \n",
    "    This is the gym wrapper class that allows our agent to interact with our environment. We need\n",
    "    to implement four main methods: step(), reset(), render(), and close(). We should also define\n",
    "    the action_space and observation space as class members.\n",
    "    \n",
    "    Note: on Windows, time.sleep() is only accurate to around 10ms. As a result, setting fps_limit\n",
    "    will give you a \"best effort\" limit.\n",
    "    \n",
    "    More information: https://gymnasium.farama.org/api/env/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        serial_port,\n",
    "        baud_rate,\n",
    "        ctrl_timeout=1.0,\n",
    "        debug_level=DebugLevel.DEBUG_NONE,\n",
    "        env_timeout=0.0, \n",
    "        stp_mode=STEP_MODE_8, \n",
    "        stp_blocking=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Set up the environment, action, and observation shapes. Optional tiemout in seconds.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call superclass's constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # Connect to Arduino board\n",
    "        self.ctrl = ControlComms(timeout=ctrl_timeout, debug_level=debug_level)\n",
    "        try:\n",
    "            self.ctrl.close()\n",
    "        except:\n",
    "            pass\n",
    "        ret = self.ctrl.connect(serial_port, baud_rate)\n",
    "        if ret is not StatusCode.OK:\n",
    "            print(\"ERROR: Could not connect to board\")\n",
    "        \n",
    "        # Define action space (scalar signifying how many degrees to move stepper by)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=STP_MOVE_MIN,\n",
    "            high=STP_MOVE_MAX,\n",
    "            shape=(1, 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Define observation space \n",
    "        # [encoder angle, encoder angular velocity, stepper angle, stepper angular velocity]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([-180, -np.inf, STP_ANGLE_MIN, -np.inf]),\n",
    "            high=np.array([180, np.inf, STP_ANGLE_MAX, np.inf]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Record time from microcontroller and own elapsed time\n",
    "        self.timestamp = 0\n",
    "        self.timeout = env_timeout\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Record previous encoder and stepper angles (to calculate velocities)\n",
    "        self.angle_stp_prev = 0\n",
    "        self.angle_enc_prev = 0\n",
    "        \n",
    "        # Set current stepper position as \"home\" and optionally set blocking\n",
    "        self.ctrl.step(CMD_SET_STEP_MODE, [stp_mode])\n",
    "        self.ctrl.step(CMD_SET_HOME, [0])\n",
    "        if stp_blocking:\n",
    "            self.ctrl.step(CMD_SET_BLOCK_MODE, [1])\n",
    "        else:\n",
    "            self.ctrl.step(CMD_SET_BLOCK_MODE, [0])\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Destructor: make sure to close the serial port\n",
    "        \"\"\"\n",
    "        self.close()\n",
    "    \n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        What happens when you tell the stepper motor to do something then record the observation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize return values\n",
    "        obs = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        reward = 0.0\n",
    "        info = {\"error\": False, \"dtime\": 0.0, \"elapesed_time\": 0.0}\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        # Box is 2D NumPy array, action must be sent out as 1D list [...]\n",
    "        action_list = action.flatten().tolist()\n",
    "        \n",
    "        # Move the stepper motor and wait for a response\n",
    "        resp = self.ctrl.step(CMD_MOVE_BY, action_list)\n",
    "        if resp:\n",
    "            status, timestamp, terminated, angles = resp\n",
    "            \n",
    "            # Compute lapsed time from previous observation\n",
    "            info[\"dtime\"] = timestamp - self.timestamp\n",
    "            self.timestamp = timestamp\n",
    "            \n",
    "            # Calculate velocities\n",
    "            dtheta = (angles[0] - self.angle_enc_prev) / info[\"dtime\"]\n",
    "            dphi = (angles[1] - self.angle_stp_prev) / info[\"dtime\"]\n",
    "            \n",
    "            # Construct observation\n",
    "            obs[0] = angles[0] - ENC_OFFSET\n",
    "            obs[1] = dtheta\n",
    "            obs[2] = angles[1]\n",
    "            obs[3] = dphi\n",
    "                    \n",
    "            # Calculate reward\n",
    "            if (obs[2] >= STP_ANGLE_MIN) and (obs[2] <= STP_ANGLE_MAX):\n",
    "                reward = -1 * (K_T * obs[0] ** 2 + \n",
    "                               K_DT * obs[1] ** 2 + \n",
    "                               K_P * obs[2] ** 2 +\n",
    "                               K_DP * obs[3] ** 2)\n",
    "            \n",
    "            # Stepper motor is out of bounds--terminate episode\n",
    "            else:\n",
    "                reward = REWARD_OOB\n",
    "                terminated = True\n",
    "        \n",
    "        # Something is wrong with communication\n",
    "        else:\n",
    "            print(\"ERROR: Could not communicate with Arduino\")\n",
    "            info[\"error\"] = True\n",
    "            terminated = True\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        info[\"elapsed_time\"] = time.time() - self.start_time\n",
    "        \n",
    "        # Check if we've exceeded the time limit\n",
    "        if not terminated and self.timeout > 0.0 and info[\"elapsed_time\"] >= self.timeout:\n",
    "            truncated = True\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Return the pendulum to the starting position\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize return values\n",
    "        obs = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        info = {\"error\": False, \"dtime\": 0, \"elapsed_time\": 0.0}\n",
    "        \n",
    "        # Reset timer\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Let the pendulum fall and return to the starting position\n",
    "        time.sleep(RESET_SETTLE_TIME)\n",
    "        resp = self.ctrl.step(CMD_MOVE_TO, [0.0])\n",
    "        if resp:\n",
    "            status, timestamp, terminated, angles = resp\n",
    "            \n",
    "            # Compute lapsed time from previous observation\n",
    "            info[\"dtime\"] = timestamp - self.timestamp\n",
    "            self.timestamp = timestamp\n",
    "            \n",
    "            # Calculate velocities\n",
    "            dtheta = (angles[0] - self.angle_enc_prev) / info[\"dtime\"]\n",
    "            dphi = (angles[1] - self.angle_stp_prev) / info[\"dtime\"]\n",
    "            \n",
    "            # Construct observation\n",
    "            obs[0] = angles[0] - ENC_OFFSET\n",
    "            obs[1] = dtheta\n",
    "            obs[2] = angles[1]\n",
    "            obs[3] = dphi\n",
    "            time.sleep(RESET_SETTLE_TIME)\n",
    "            \n",
    "        # Something is wrong with communication\n",
    "        else:\n",
    "            print(\"ERROR: Could not communicate with Arduino\")\n",
    "            info[\"error\"] = True\n",
    "            \n",
    "        # Calculate elapsed time\n",
    "        info[\"elapsed_time\"] = time.time() - self.start_time\n",
    "        \n",
    "        return obs, info\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close connection to Arduino\n",
    "        \"\"\"\n",
    "        self.ctrl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac966999-8595-4e98-8a34-60ad251ce69c",
   "metadata": {},
   "source": [
    "## Test gym Environment\n",
    "\n",
    "Test the gym wrapper before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b6c823-05f7-4368-815d-353bd1acf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env = Pendulum(\n",
    "        SERIAL_PORT,\n",
    "        BAUD_RATE,\n",
    "        ctrl_timeout=CTRL_TIMEOUT,\n",
    "        debug_level=DEBUG_LEVEL,\n",
    "        env_timeout=ENV_TIMEOUT, \n",
    "        stp_mode=STEP_MODE, \n",
    "        stp_blocking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "656213f8-6fa8-4a41-828a-0761a601b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step   |           Observation            |      Reward      |   Done   | Info\n",
      " Reset   | 168.90, 0.02, 0.00, 0.00         | 0.0              |  False   | {'error': False, 'dtime': 19225, 'elapsed_time': 2.013803482055664}\n",
      "   0     | 177.00, 0.32, 0.00, 0.00         | -31329.01        |  False   | {'error': False, 'dtime': 1109, 'elapesed_time': 0.0, 'elapsed_time': 2.1107232570648193}\n",
      "   1     | -157.50, 0.23, -24.97, -0.25     | -24812.49        |  False   | {'error': False, 'dtime': 99, 'elapesed_time': 0.0, 'elapsed_time': 2.2053277492523193}\n",
      "   2     | -140.40, 0.42, -49.95, -0.53     | -19737.13        |  False   | {'error': False, 'dtime': 95, 'elapesed_time': 0.0, 'elapsed_time': 2.299238443374634}\n",
      "   3     | -135.60, 0.47, -74.93, -0.79     | -18443.53        |  False   | {'error': False, 'dtime': 95, 'elapesed_time': 0.0, 'elapsed_time': 2.396799325942993}\n",
      "   4     | -144.60, 0.36, -99.90, -1.01     | -21008.98        |  False   | {'error': False, 'dtime': 99, 'elapesed_time': 0.0, 'elapsed_time': 2.493980884552002}\n",
      "   5     | -163.80, 0.17, -124.87, -1.27    | -26986.37        |  False   | {'error': False, 'dtime': 98, 'elapesed_time': 0.0, 'elapsed_time': 2.5909409523010254}\n",
      "   6     | 173.40, 3.61, -149.85, -1.53     | -30293.41        |  False   | {'error': False, 'dtime': 98, 'elapesed_time': 0.0, 'elapsed_time': 2.6876111030578613}\n",
      "   7     | 154.80, 3.42, -174.83, -1.78     | -24269.87        |  False   | {'error': False, 'dtime': 98, 'elapesed_time': 0.0, 'elapsed_time': 2.784992218017578}\n",
      "   8     | 145.20, 3.32, -199.80, -2.04     | -10000.00        |   True   | {'error': False, 'dtime': 98, 'elapesed_time': 0.0, 'elapsed_time': 2.8803517818450928}\n",
      "Episode done\n"
     ]
    }
   ],
   "source": [
    "# Try running the environment for a few steps\n",
    "obs, info = env.reset()\n",
    "obs_str = \", \".join([f\"{val:.2f}\" for val in obs])\n",
    "if info[\"error\"]:\n",
    "    print(\"Stopping\")\n",
    "else:\n",
    "    print(f\"{'Step': ^8} | {'Observation': ^32} | {'Reward': ^16} | {'Done': ^8} | Info\")\n",
    "    print(f\"{'Reset': ^8} | {obs_str: <32} | {0.0: <16} | {str(False): ^8} | {info}\")\n",
    "    for i in range(10):\n",
    "        obs, reward, terminated, truncated, info = env.step(np.array([[-25]]))\n",
    "        obs_str = \", \".join([f\"{val:.2f}\" for val in obs])\n",
    "        print(f\"{i: ^8} | {obs_str: <32} | {reward: <16.2f} | {str(terminated or truncated): ^8} | {info}\")\n",
    "        if info[\"error\"]:\n",
    "            print(\"Stopping\")\n",
    "            break\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode done\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b1cc85-0cc9-496e-93ba-4f670de7c4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done\n"
     ]
    }
   ],
   "source": [
    "# Test timeout\n",
    "obs, info = env.reset()\n",
    "action = 2\n",
    "if not info[\"error\"]:\n",
    "    for _ in range(1000):\n",
    "        action = -2 if action == 2 else 2\n",
    "        obs, reward, terminated, truncated, info = env.step(np.array([[action]]))\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode done\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c8710f-6a1b-4e54-ab1c-82cbc4d73080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgmustadio\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:428: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Final environment check to make sure it works with Stable-Baselines3\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f593d48-dacf-424b-ae3c-531b2d5914c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a13f9-7fd8-41b8-a132-7ba226d43345",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec36bf65-a179-40e6-9543-10ce60d66b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env = Pendulum(\n",
    "        SERIAL_PORT,\n",
    "        BAUD_RATE,\n",
    "        ctrl_timeout=CTRL_TIMEOUT,\n",
    "        debug_level=DEBUG_LEVEL,\n",
    "        env_timeout=ENV_TIMEOUT, \n",
    "        stp_mode=STEP_MODE, \n",
    "        stp_blocking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f6d7db-4f12-400a-b61e-b726e8f028a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that tests the model in the given environment\n",
    "def test_model(env, model):\n",
    "\n",
    "    # Reset environment\n",
    "    obs, info = env.reset()\n",
    "    ep_len = 0\n",
    "    ep_rew = 0\n",
    "\n",
    "    # Run episode until complete\n",
    "    while True:\n",
    "\n",
    "        # Provide observation to policy to predict the next action\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Perform action, update total reward\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_rew += reward\n",
    "\n",
    "        # Increase step counter\n",
    "        ep_len += 1\n",
    "\n",
    "        # Check to see if episode has ended\n",
    "        if terminated or truncated:\n",
    "            return ep_len, ep_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "633eb05f-9a85-4573-83ad-cf711c9c2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# PPO docs: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "# Policy networks: https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
    "# Hyperparameters from: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
    "model = sb3.PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=0.001,       # Learning rate of neural network (default: 0.0003)\n",
    "    n_steps=1024,               # Number of steps per update (default: 2048)\n",
    "    batch_size=64,              # Minibatch size for NN update (default: 64)\n",
    "    gamma=0.9,                 # Discount factor (default: 0.99)\n",
    "    ent_coef=0.0,               # Entropy, how much to explore (default: 0.0)\n",
    "    use_sde=True,               # Use generalized State Dependent Exploration (default: False)\n",
    "    sde_sample_freq=4,          # Number of steps before sampling new noise matrix (default -1)\n",
    "    policy_kwargs={'net_arch': [64, 64]}, # 2 hidden layers, 1 output layer (default: [64, 64])\n",
    "    verbose=0                   # Print training metrics (default: 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3974f14f-45d3-4a86-90ae-f8df2a9baef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 | average test length: 143.01, average test reward: -2070948.0657016016\n",
      "Round 1 | average test length: 227.12, average test reward: -6856389.350233657\n",
      "Round 2 | average test length: 133.32, average test reward: -3321301.3769947374\n",
      "Round 3 | average test length: 344.7, average test reward: -3820116.4364356752\n",
      "Round 4 | average test length: 33.28, average test reward: -644523.9924307623\n",
      "Round 5 | average test length: 56.74, average test reward: -1593507.5790542322\n",
      "Round 6 | average test length: 35.83, average test reward: -654718.3901519384\n",
      "Round 7 | average test length: 135.86, average test reward: -3355535.89865323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m avg_ep_rews \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rnd \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_ROUNDS):\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TRAINING_STEPS_PER_ROUND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_FILENAME_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrnd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[9], line 156\u001b[0m, in \u001b[0;36mPendulum.reset\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Let the pendulum fall and return to the starting position\u001b[39;00m\n\u001b[0;32m    155\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(RESET_SETTLE_TIME)\n\u001b[1;32m--> 156\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCMD_MOVE_TO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp:\n\u001b[0;32m    158\u001b[0m     status, timestamp, terminated, angles \u001b[38;5;241m=\u001b[39m resp\n",
      "File \u001b[1;32mD:\\Projects\\GitHub\\pendulum-rl\\control_comms.py:247\u001b[0m, in \u001b[0;36mControlComms.step\u001b[1;34m(self, command, action)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Wait for a response\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_until\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_level \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m DebugLevel\u001b[38;5;241m.\u001b[39mDEBUG_ERROR:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\serial\\serialutil.py:663\u001b[0m, in \u001b[0;36mSerialBase.read_until\u001b[1;34m(self, expected, size)\u001b[0m\n\u001b[0;32m    661\u001b[0m timeout \u001b[38;5;241m=\u001b[39m Timeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout)\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c:\n\u001b[0;32m    665\u001b[0m         line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m c\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\serial\\serialwin32.py:288\u001b[0m, in \u001b[0;36mSerial.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m read_ok \u001b[38;5;129;01mand\u001b[39;00m win32\u001b[38;5;241m.\u001b[39mGetLastError() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (win32\u001b[38;5;241m.\u001b[39mERROR_SUCCESS, win32\u001b[38;5;241m.\u001b[39mERROR_IO_PENDING):\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SerialException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFile failed (\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ctypes\u001b[38;5;241m.\u001b[39mWinError()))\n\u001b[1;32m--> 288\u001b[0m result_ok \u001b[38;5;241m=\u001b[39m \u001b[43mwin32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetOverlappedResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_port_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_overlapped_read\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_ok:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m win32\u001b[38;5;241m.\u001b[39mGetLastError() \u001b[38;5;241m!=\u001b[39m win32\u001b[38;5;241m.\u001b[39mERROR_OPERATION_ABORTED:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and testing hyperparameters\n",
    "NUM_ROUNDS = 20\n",
    "NUM_TRAINING_STEPS_PER_ROUND = 5000\n",
    "NUM_TESTS_PER_ROUND = 100\n",
    "MODEL_FILENAME_BASE = \"inverted-pendulum-ppo\"\n",
    "\n",
    "# Train and test the model for a number of rounds\n",
    "avg_ep_lens = []\n",
    "avg_ep_rews = []\n",
    "for rnd in range(NUM_ROUNDS):\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=NUM_TRAINING_STEPS_PER_ROUND)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f\"{MODEL_FILENAME_BASE}_{rnd}\")\n",
    "\n",
    "    # Test the model in several episodes\n",
    "    avg_ep_len = 0\n",
    "    avg_ep_rew = 0\n",
    "    for ep in range(NUM_TESTS_PER_ROUND):\n",
    "        ep_len, ep_rew = test_model(env, model)\n",
    "        avg_ep_len += ep_len\n",
    "        avg_ep_rew += ep_rew\n",
    "\n",
    "    # Record and dieplay average episode length and reward\n",
    "    avg_ep_len /= NUM_TESTS_PER_ROUND\n",
    "    avg_ep_lens.append(avg_ep_len)\n",
    "    avg_ep_rew /= NUM_TESTS_PER_ROUND\n",
    "    avg_ep_rews.append(avg_ep_rew)\n",
    "    print(f\"Round {rnd} | average test length: {avg_ep_len}, average test reward: {avg_ep_rew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5de3-822d-4395-b1e8-2537680b0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test lengths and rewards\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b3fb4-2186-425e-8079-a54735dde9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
