{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4561acc1-bea2-4e8b-8342-e1de5391f578",
   "metadata": {},
   "source": [
    "TODO:\n",
    " * Stepper motor skipping steps--more current (better power supply)?\n",
    " * Normalize action and observation spaces (see: https://ai.stackexchange.com/questions/21477/why-do-we-also-need-to-normalize-the-actions-values-on-continuous-action-spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37363014-d5bd-4d0b-8093-ad182644070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install gymnasium==0.28.1\n",
    "# !python -m pip install stable-baselines3[extra]==2.1.0\n",
    "# !python -m pip install ax-platform==0.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ed8b9b-73a0-4c11-9c12-4bbd6b0debfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.0\n",
      "gymnasium version: 0.28.1\n",
      "sb3 version: 2.1.0\n",
      "cv2 version: 4.7.0.68\n",
      "ax version: 0.3.4\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "import importlib.metadata\n",
    "\n",
    "print(f\"torch version: {importlib.metadata.version('torch')}\")\n",
    "print(f\"gymnasium version: {importlib.metadata.version('gymnasium')}\")\n",
    "print(f\"sb3 version: {importlib.metadata.version('stable-baselines3')}\")\n",
    "print(f\"cv2 version: {importlib.metadata.version('opencv-python')}\")\n",
    "print(f\"ax version: {importlib.metadata.version('ax-platform')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6601a351-afc4-4ad6-8119-335dce1392cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "# Encoder and stepper controls (local)\n",
    "from control_comms import ControlComms, StatusCode, DebugLevel\n",
    "\n",
    "# Third-party packages\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# Reinforcement model modules\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import KVWriter, Logger\n",
    "\n",
    "# Meta Ax\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render\n",
    "from ax.utils.tutorials.cnn_utils import train, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002868a4-1404-46fb-88a0-80adf9ee9dc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5512c5e3-f346-4866-be07-c9f275fc1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication settings\n",
    "SERIAL_PORT = \"COM6\"    # Check your devices\n",
    "BAUD_RATE = 500000      # Must match what's in the Arduino code!\n",
    "CTRL_TIMEOUT = 1.0      # Seconds\n",
    "DEBUG_LEVEL = DebugLevel.DEBUG_ERROR\n",
    "\n",
    "# Reinforcement learning settings\n",
    "K_T = 5                 # Reward constant to multiply theta (angle of encoder)\n",
    "K_DT = 0.05             # Reward constant to multiply dtheto/dt (angular velocity of encoder)\n",
    "K_P = 2                 # Reward constant to multiply phi (angle of stepper)\n",
    "K_DP = 0.05             # Reward constant to multiply dphi/dt (angular velocity of stepper)\n",
    "REWARD_OOB = -500       # Reward (penalty) for having the stepper motor move out of bounds (OOB)\n",
    "ENC_ANGLE_NORM = 180    # Divide by this to normalize +/-180 deg angle to +/-1\n",
    "STP_MOVE_MIN = -10.0\n",
    "STP_MOVE_MAX = 10.0\n",
    "STP_ANGLE_MIN = -180    # Episode ends if stepper goes beyond this angle\n",
    "STP_ANGLE_MAX = 180     # Episode ends if stepper goes beyond this angle\n",
    "STP_ANGLE_NORM = 180    # Divide by this to normalize +/-180 deg angle to +/-1\n",
    "\n",
    "ENV_TIMEOUT = 10.0\n",
    "RESET_SETTLE_TIME = 2.0 # Seconds to wait after reset to start moving again\n",
    "\n",
    "# Angle constants\n",
    "ENC_OFFSET = 180.0      # Pendulum in the \"up\" position should be 0 deg\n",
    "ANG_REV = 360           # Degrees in a single revolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631b6b53-ca9c-41c4-858c-581fcfbbe524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication constants\n",
    "CMD_SET_HOME = 0        # Set current stepper position as home (0 deg)\n",
    "CMD_MOVE_TO = 1         # Move stepper to a particular position (deg)\n",
    "CMD_MOVE_BY = 2         # Move stepper by a given amount (deg)\n",
    "CMD_SET_STEP_MODE = 3   # Set step mode\n",
    "CMD_SET_BLOCK_MODE = 4  # Set blocking mode\n",
    "CMD_NOP = 5             # Take no action, just receive observation\n",
    "STEP_MODE_1 = 0         # 1 division per step\n",
    "STEP_MODE_2 = 1         # 2 divisions per step\n",
    "STEP_MODE_4 = 2         # 4 divisions per step\n",
    "STEP_MODE_8 = 3         # 8 divisions per step\n",
    "STEP_MODE_16 = 4        # 16 divisions per step\n",
    "STATUS_OK = 0           # Stepper idle\n",
    "STATUS_STP_MOVING = 1   # Stepper is currently moving\n",
    "\n",
    "# Set to desired step mode\n",
    "STEP_MODE = STEP_MODE_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00724565-b597-4c6a-a8fd-9dec4114949b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316bc585-a495-4ad3-ae94-6b0930a38bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection to Arduino board (if open)\n",
    "try:\n",
    "    controller.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd6f5fef-12a0-405f-972a-fc6eb40b2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Arduino board\n",
    "controller = ControlComms(timeout=CTRL_TIMEOUT, debug_level=DEBUG_LEVEL)\n",
    "ret = controller.connect(SERIAL_PORT, BAUD_RATE)\n",
    "if ret is not StatusCode.OK:\n",
    "    print(\"ERROR: Could not connect to board\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f80790-322b-498b-bef8-211d8a2696d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 128753, False, [358.5, 0.22])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test basic comms\n",
    "controller.step(CMD_SET_STEP_MODE, [STEP_MODE_8])\n",
    "controller.step(CMD_SET_HOME, [0])\n",
    "controller.step(CMD_SET_BLOCK_MODE, [1])\n",
    "controller.step(CMD_MOVE_BY, [90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c07c94e-8ab6-490f-b856-ca817352049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close comms\n",
    "controller.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf62f43-718b-4997-b112-86c364a3f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshawnhymel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3cce2f0-188f-40af-9282-ea58bb80e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make wandb be quiet\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "logger = logging.getLogger(\"wandb\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566ff73-1195-4710-bab6-05a8309f7fa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6800b3d-197b-465f-b09b-b451708a16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Seed the different random generators.\n",
    "    https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/utils.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set seed for Python random and NumPy\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bb2c64-36e7-41cc-9c86-67b35d741ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angular_velocity(ang, ang_prev, dt):\n",
    "    \"\"\"\n",
    "    Estimate engular velocity based on current and previous readings. Note that we assume that the\n",
    "    object in question cannot go the long way around (e.g. more than 180 deg).\n",
    "    \"\"\"\n",
    "    da = ang - ang_prev\n",
    "    if da > (ANG_REV / 2):\n",
    "        da -= ANG_REV\n",
    "    elif da < -(ANG_REV / 2):\n",
    "        da += ANG_REV\n",
    "    \n",
    "    return da / dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cd180-65fe-40f2-88bf-e584bdbbd2cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build gym Environment\n",
    "\n",
    "Subclass gymnasium.Env to create a custom environment. Learn more here:<br>\n",
    "https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32bcddaa-b899-4cab-9a7b-82bd214c3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulum(gym.Env):\n",
    "    \"\"\"\n",
    "    Subclass gymnasium Env class\n",
    "    \n",
    "    This is the gym wrapper class that allows our agent to interact with our environment. We need\n",
    "    to implement four main methods: step(), reset(), render(), and close(). We should also define\n",
    "    the action_space and observation space as class members.\n",
    "    \n",
    "    Note: on Windows, time.sleep() is only accurate to around 10ms. As a result, setting fps_limit\n",
    "    will give you a \"best effort\" limit.\n",
    "    \n",
    "    More information: https://gymnasium.farama.org/api/env/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        serial_port,\n",
    "        baud_rate,\n",
    "        ctrl_timeout=1.0,\n",
    "        debug_level=DebugLevel.DEBUG_NONE,\n",
    "        env_timeout=0.0, \n",
    "        stp_mode=STEP_MODE_8, \n",
    "        stp_blocking=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Set up the environment, action, and observation shapes. Optional tiemout in seconds.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call superclass's constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # Connect to Arduino board\n",
    "        self.ctrl = ControlComms(timeout=ctrl_timeout, debug_level=debug_level)\n",
    "        try:\n",
    "            self.ctrl.close()\n",
    "        except:\n",
    "            pass\n",
    "        ret = self.ctrl.connect(serial_port, baud_rate)\n",
    "        if ret is not StatusCode.OK:\n",
    "            print(\"ERROR: Could not connect to board\")\n",
    "        \n",
    "        # Define action space (scalar signifying how many degrees to move stepper by)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=STP_MOVE_MIN,\n",
    "            high=STP_MOVE_MAX,\n",
    "            shape=(1, 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Define observation space \n",
    "        # [encoder angle, encoder angular velocity, stepper angle, stepper angular velocity]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([-180, -np.inf, STP_ANGLE_MIN, -np.inf]),\n",
    "            high=np.array([180, np.inf, STP_ANGLE_MAX, np.inf]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Record time from microcontroller and own elapsed time\n",
    "        self.timestamp = 0\n",
    "        self.timeout = env_timeout\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Record previous encoder and stepper angles (to calculate velocities)\n",
    "        self.angle_stp_prev = 0\n",
    "        self.angle_enc_prev = 0\n",
    "        \n",
    "        # Set current stepper position as \"home\" and optionally set blocking\n",
    "        self.ctrl.step(CMD_SET_STEP_MODE, [stp_mode])\n",
    "        self.ctrl.step(CMD_SET_HOME, [0])\n",
    "        if stp_blocking:\n",
    "            self.ctrl.step(CMD_SET_BLOCK_MODE, [1])\n",
    "        else:\n",
    "            self.ctrl.step(CMD_SET_BLOCK_MODE, [0])\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Destructor: make sure to close the serial port\n",
    "        \"\"\"\n",
    "        self.close()\n",
    "    \n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        What happens when you tell the stepper motor to do something then record the observation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize return values\n",
    "        obs = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        reward = 0.0\n",
    "        info = {\"error\": False, \"dtime\": 0.0, \"elapsed_time\": 0.0}\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        # Box is 2D NumPy array, action must be sent out as 1D list [...]\n",
    "        action_list = action.flatten().tolist()\n",
    "        \n",
    "        # Move the stepper motor and wait for a response\n",
    "        resp = self.ctrl.step(CMD_MOVE_BY, action_list)\n",
    "        if resp:\n",
    "            \n",
    "            # Extract information from controller response\n",
    "            status, timestamp, terminated, angles = resp\n",
    "            \n",
    "            # Compute lapsed time (in seconds) from previous observation (milliseconds)\n",
    "            info[\"dtime\"] = (timestamp - self.timestamp) / 1000.0\n",
    "            self.timestamp = timestamp\n",
    "            \n",
    "            # Offset encoder angle so that 0 deg is up\n",
    "            angles[0] -= ENC_OFFSET\n",
    "            \n",
    "            # Calculate velocities\n",
    "            dtheta = calc_angular_velocity(angles[0], self.angle_enc_prev, info['dtime'])\n",
    "            dphi = calc_angular_velocity(angles[1], self.angle_stp_prev, info['dtime'])\n",
    "            self.angle_enc_prev = angles[0]\n",
    "            self.angle_stp_prev = angles[1]\n",
    "            \n",
    "            # Construct observation (normalized)\n",
    "            obs[0] = angles[0] / ENC_ANGLE_NORM\n",
    "            obs[1] = dtheta / ENC_ANGLE_NORM\n",
    "            obs[2] = angles[1] / STP_ANGLE_NORM\n",
    "            obs[3] = dphi / STP_ANGLE_NORM\n",
    "                    \n",
    "            # Calculate reward if stepper is not out of bounds\n",
    "            if (angles[1] >= STP_ANGLE_MIN) and (angles[1] <= STP_ANGLE_MAX):\n",
    "                reward = -1 * (K_T * obs[0] ** 2 + \n",
    "                               K_DT * obs[1] ** 2 + \n",
    "                               K_P * obs[2] ** 2 +\n",
    "                               K_DP * obs[3] ** 2)\n",
    "            \n",
    "            # Stepper motor is out of bounds--terminate episode\n",
    "            else:\n",
    "                reward = REWARD_OOB\n",
    "                terminated = True\n",
    "        \n",
    "        # Something is wrong with communication\n",
    "        else:\n",
    "            print(\"ERROR: Could not communicate with Arduino\")\n",
    "            info[\"error\"] = True\n",
    "            terminated = True\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        info[\"elapsed_time\"] = time.time() - self.start_time\n",
    "        \n",
    "        # Check if we've exceeded the time limit\n",
    "        if not terminated and self.timeout > 0.0 and info[\"elapsed_time\"] >= self.timeout:\n",
    "            truncated = True\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Return the pendulum to the starting position\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize return values\n",
    "        obs = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        info = {\"error\": False, \"dtime\": 0, \"elapsed_time\": 0.0}\n",
    "        \n",
    "        # Reset timer\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Let the pendulum fall and return to the starting position\n",
    "        time.sleep(RESET_SETTLE_TIME)\n",
    "        resp = self.ctrl.step(CMD_MOVE_TO, [0.0])\n",
    "        if resp:\n",
    "            \n",
    "            # Extract information from controller response\n",
    "            status, timestamp, terminated, angles = resp\n",
    "            \n",
    "            # Compute lapsed time (in seconds) from previous observation (milliseconds)\n",
    "            info[\"dtime\"] = (timestamp - self.timestamp) / 1000.0\n",
    "            self.timestamp = timestamp\n",
    "            \n",
    "            # Offset encoder angle so that 0 deg is up\n",
    "            angles[0] -= ENC_OFFSET\n",
    "            \n",
    "            # Calculate velocities\n",
    "            dtheta = calc_angular_velocity(angles[0], self.angle_enc_prev, info['dtime'])\n",
    "            dphi = calc_angular_velocity(angles[1], self.angle_stp_prev, info['dtime'])\n",
    "            self.angle_enc_prev = angles[0]\n",
    "            self.angle_stp_prev = angles[1]\n",
    "            \n",
    "            # Construct observation (normalized)\n",
    "            obs[0] = angles[0] / ENC_ANGLE_NORM\n",
    "            obs[1] = dtheta / ENC_ANGLE_NORM\n",
    "            obs[2] = angles[1] / STP_ANGLE_NORM\n",
    "            obs[3] = dphi / STP_ANGLE_NORM\n",
    "            \n",
    "            # Let pendulum settle for a bit\n",
    "            time.sleep(RESET_SETTLE_TIME)\n",
    "            \n",
    "        # Something is wrong with communication\n",
    "        else:\n",
    "            print(\"ERROR: Could not communicate with Arduino\")\n",
    "            info[\"error\"] = True\n",
    "            \n",
    "        # Calculate elapsed time\n",
    "        info[\"elapsed_time\"] = time.time() - self.start_time\n",
    "        \n",
    "        return obs, info\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close connection to Arduino\n",
    "        \"\"\"\n",
    "        self.ctrl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac966999-8595-4e98-8a34-60ad251ce69c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test gym Environment\n",
    "\n",
    "Test the gym wrapper before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79b6c823-05f7-4368-815d-353bd1acf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env = Pendulum(\n",
    "        SERIAL_PORT,\n",
    "        BAUD_RATE,\n",
    "        ctrl_timeout=CTRL_TIMEOUT,\n",
    "        debug_level=DEBUG_LEVEL,\n",
    "        env_timeout=ENV_TIMEOUT, \n",
    "        stp_mode=STEP_MODE, \n",
    "        stp_blocking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "656213f8-6fa8-4a41-828a-0761a601b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step   |             Observation              |  Reward  |   Done   | Info\n",
      " Reset   | -1.00, -0.00, 0.00, 0.00             | 0.0      |  False   | {'error': False, 'dtime': 35334.422, 'elapsed_time': 4.012627124786377}\n",
      "   0     | -1.00, 0.00, 0.00, 0.00              | -4.98    |  False   | {'error': False, 'dtime': 2.127, 'elapsed_time': 4.109713315963745}\n",
      "   1     | -0.88, 1.14, -0.14, -1.40            | -4.12    |  False   | {'error': False, 'dtime': 0.099, 'elapsed_time': 4.207722425460815}\n",
      "   2     | -0.81, 0.72, -0.28, -1.40            | -3.59    |  False   | {'error': False, 'dtime': 0.099, 'elapsed_time': 4.3066840171813965}\n",
      "   3     | -0.80, 0.17, -0.42, -1.39            | -3.62    |  False   | {'error': False, 'dtime': 0.1, 'elapsed_time': 4.4027793407440186}\n",
      "   4     | -0.92, -1.26, -0.56, -1.46           | -5.00    |  False   | {'error': False, 'dtime': 0.095, 'elapsed_time': 4.496731519699097}\n",
      "   5     | -0.98, -0.63, -0.69, -1.46           | -5.86    |  False   | {'error': False, 'dtime': 0.095, 'elapsed_time': 4.593378782272339}\n",
      "   6     | 0.96, -0.64, -0.83, -1.40            | -6.11    |  False   | {'error': False, 'dtime': 0.099, 'elapsed_time': 4.689304828643799}\n",
      "   7     | 0.91, -0.54, -0.97, -1.45            | -6.13    |  False   | {'error': False, 'dtime': 0.096, 'elapsed_time': 4.7832465171813965}\n",
      "   8     | 0.88, -0.35, -1.11, -1.46            | -500.00  |   True   | {'error': False, 'dtime': 0.095, 'elapsed_time': 4.8807759284973145}\n",
      "Episode done\n"
     ]
    }
   ],
   "source": [
    "# Try running the environment for a few steps (stepper should move some)\n",
    "obs, info = env.reset()\n",
    "obs_str = \", \".join([f\"{val:.2f}\" for val in obs])\n",
    "if info[\"error\"]:\n",
    "    print(\"Stopping\")\n",
    "else:\n",
    "    print(f\"{'Step': ^8} | {'Observation': ^36} | {'Reward': ^8} | {'Done': ^8} | Info\")\n",
    "    print(f\"{'Reset': ^8} | {obs_str: <36} | {0.0: <8} | {str(False): ^8} | {info}\")\n",
    "    for i in range(10):\n",
    "        obs, reward, terminated, truncated, info = env.step(np.array([[-25]]))\n",
    "        obs_str = \", \".join([f\"{val:.2f}\" for val in obs])\n",
    "        print(f\"{i: ^8} | {obs_str: <36} | {reward: <8.2f} | {str(terminated or truncated): ^8} | {info}\")\n",
    "        if info[\"error\"]:\n",
    "            print(\"Stopping\")\n",
    "            break\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode done\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3b1cc85-0cc9-496e-93ba-4f670de7c4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done\n"
     ]
    }
   ],
   "source": [
    "# Test timeout (stepper will reset and vibrate for a while)\n",
    "obs, info = env.reset()\n",
    "action = 2\n",
    "if not info[\"error\"]:\n",
    "    for _ in range(1000):\n",
    "        action = -2 if action == 2 else 2\n",
    "        obs, reward, terminated, truncated, info = env.step(np.array([[action]]))\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode done\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c8710f-6a1b-4e54-ab1c-82cbc4d73080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgmustadio\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:428: UserWarning:\n",
      "\n",
      "We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final environment check to make sure it works with Stable-Baselines3 (no errors means it worked)\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f593d48-dacf-424b-ae3c-531b2d5914c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba49c8d3-bc60-4b99-be19-6aa044fde405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step   |             Observation              |  Reward  |   Done   | Info\n",
      " Reset   | -0.92, -0.00, 0.00, 0.00             | 0.0      |  False   | {'error': False, 'dtime': 35360.702, 'elapsed_time': 4.022712707519531}\n",
      "   0     | 0.97, -0.06, 0.00, 0.00              | -4.67    |  False   | {'error': False, 'dtime': 2.039, 'elapsed_time': 4.026947259902954}\n",
      "   1     | 0.97, 0.00, 0.00, 0.00               | -4.69    |  False   | {'error': False, 'dtime': 1.017, 'elapsed_time': 5.0331807136535645}\n",
      "   2     | 0.46, -0.50, 0.00, 0.00              | -1.06    |  False   | {'error': False, 'dtime': 1.019, 'elapsed_time': 6.043116092681885}\n",
      "   3     | 0.49, 0.03, 0.00, 0.00               | -1.19    |  False   | {'error': False, 'dtime': 1.024, 'elapsed_time': 7.054731369018555}\n",
      "   4     | 0.02, -0.46, 0.00, 0.00              | -0.01    |  False   | {'error': False, 'dtime': 1.017, 'elapsed_time': 8.060448169708252}\n",
      "   5     | 0.01, -0.01, 0.00, 0.00              | -0.00    |  False   | {'error': False, 'dtime': 1.019, 'elapsed_time': 9.071463823318481}\n",
      "   6     | -0.37, -0.37, 0.00, 0.00             | -0.69    |  False   | {'error': False, 'dtime': 1.033, 'elapsed_time': 10.090928077697754}\n",
      "   7     | -0.40, -0.03, 0.00, 0.00             | -0.81    |  False   | {'error': False, 'dtime': 1.021, 'elapsed_time': 11.103286743164062}\n",
      "   8     | -0.43, -0.03, 0.00, 0.00             | -0.92    |  False   | {'error': False, 'dtime': 1.019, 'elapsed_time': 12.111396074295044}\n",
      "   9     | -0.48, -0.05, 0.00, 0.00             | -1.14    |  False   | {'error': False, 'dtime': 1.02, 'elapsed_time': 13.116913795471191}\n"
     ]
    }
   ],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env = Pendulum(\n",
    "        SERIAL_PORT,\n",
    "        BAUD_RATE,\n",
    "        ctrl_timeout=CTRL_TIMEOUT,\n",
    "        debug_level=DEBUG_LEVEL,\n",
    "        env_timeout=0, \n",
    "        stp_mode=STEP_MODE, \n",
    "        stp_blocking=True\n",
    ")\n",
    "\n",
    "# Test encoder\n",
    "obs, info = env.reset()\n",
    "obs_str = \", \".join([f\"{val:.2f}\" for val in obs])\n",
    "if info[\"error\"]:\n",
    "    print(\"Stopping\")\n",
    "else:\n",
    "    print(f\"{'Step': ^8} | {'Observation': ^36} | {'Reward': ^8} | {'Done': ^8} | Info\")\n",
    "    print(f\"{'Reset': ^8} | {obs_str: <36} | {0.0: <8} | {str(False): ^8} | {info}\")\n",
    "    for i in range(10):\n",
    "        obs, reward, terminated, truncated, info = env.step(np.array([[0]]))\n",
    "        obs_str = \", \".join([f\"{val:.2f}\" for val in obs])\n",
    "        print(f\"{i: ^8} | {obs_str: <36} | {reward: <8.2f} | {str(terminated or truncated): ^8} | {info}\")\n",
    "        if info[\"error\"]:\n",
    "            print(\"Stopping\")\n",
    "            break\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode done\")\n",
    "            break\n",
    "        time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a13f9-7fd8-41b8-a132-7ba226d43345",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec36bf65-a179-40e6-9543-10ce60d66b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env = Pendulum(\n",
    "        SERIAL_PORT,\n",
    "        BAUD_RATE,\n",
    "        ctrl_timeout=CTRL_TIMEOUT,\n",
    "        debug_level=DEBUG_LEVEL,\n",
    "        env_timeout=ENV_TIMEOUT, \n",
    "        stp_mode=STEP_MODE, \n",
    "        stp_blocking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74f6d7db-4f12-400a-b61e-b726e8f028a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that tests the model in the given environment\n",
    "def test_agent(env, model, max_steps=0):\n",
    "\n",
    "    # Reset environment\n",
    "    obs, info = env.reset()\n",
    "    ep_len = 0\n",
    "    ep_rew = 0\n",
    "    avg_step_time = 0.0\n",
    "\n",
    "    # Run episode until complete\n",
    "    while True:\n",
    "\n",
    "        # Provide observation to policy to predict the next action\n",
    "        timestamp = time.time()\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Perform action, update total reward\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        avg_step_time += time.time() - timestamp\n",
    "        ep_rew += reward\n",
    "\n",
    "        # Increase step counter\n",
    "        ep_len += 1\n",
    "        if (max_steps > 0) and (ep_len >= max_steps):\n",
    "            break\n",
    "\n",
    "        # Check to see if episode has ended\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "    # Calculate average step time\n",
    "    avg_step_time /= ep_len\n",
    "    \n",
    "    return ep_len, ep_rew, avg_step_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de81902-9bcc-451d-99fd-7db9febdd73a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing and logging callbacks\n",
    "\n",
    "Construct custom callbacks for Stable-Baselines3 to test our agent and log metrics to Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f3870bb-7d23-4d12-a171-4f284c3335e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent on a number of tests\n",
    "def evaluate_agent(env, model, steps_per_test, num_tests):\n",
    "    \n",
    "    # Initialize metrics\n",
    "    avg_ep_len = 0\n",
    "    avg_ep_rew = 0\n",
    "    avg_step_time = 0.0\n",
    "    \n",
    "    # Test the agent a number of times\n",
    "    for ep in range(num_tests):\n",
    "        ep_len, ep_rew, step_time = test_agent(env, model, max_steps=steps_per_test)\n",
    "        avg_ep_len += ep_len\n",
    "        avg_ep_rew += ep_rew\n",
    "        avg_step_time += step_time\n",
    "        \n",
    "    # Compute metrics\n",
    "    avg_ep_len /= num_tests\n",
    "    avg_ep_rew /= num_tests\n",
    "    avg_step_time /= num_tests\n",
    "    \n",
    "    return avg_ep_len, avg_ep_rew, avg_step_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e467390-4f21-48cd-a5e3-2862ddf25cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalAndSaveCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Evaluate and save the model every ``check_freq`` steps\n",
    "    \n",
    "    More info: https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self, \n",
    "        check_freq, \n",
    "        save_dir,\n",
    "        model_name=\"model\",\n",
    "        replay_buffer_name=None,\n",
    "        steps_per_test=0, \n",
    "        num_tests=10,\n",
    "        step_offset=0,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        super(EvalAndSaveCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_dir = save_dir\n",
    "        self.model_name = model_name\n",
    "        self.replay_buffer_name = replay_buffer_name\n",
    "        self.num_tests = num_tests\n",
    "        self.steps_per_test = steps_per_test\n",
    "        self.step_offset = step_offset\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    # Create directory for saving the models\n",
    "    def _init_callback(self):\n",
    "        if self.save_dir is not None:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "    # Save and evaluate model at a set interval\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            \n",
    "            # Set actual number of steps (including offset)\n",
    "            actual_steps = self.step_offset + self.n_calls\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(self.save_dir, f\"{self.model_name}_{str(actual_steps)}\")\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "            # Save replay buffer\n",
    "            if self.replay_buffer_name != None:\n",
    "                replay_buffer_path = os.path.join(self.save_dir, f\"{self.replay_buffer_name}\")\n",
    "                self.model.save_replay_buffer(replay_buffer_path)\n",
    "            \n",
    "            # Evaluate the agent\n",
    "            avg_ep_len, avg_ep_rew, avg_step_time = evaluate_agent(\n",
    "                env, \n",
    "                self.model, \n",
    "                self.steps_per_test, \n",
    "                self.num_tests\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"{str(actual_steps)} steps | average test length: {avg_ep_len}, average test reward: {avg_ep_rew}\")\n",
    "                \n",
    "            # Log metrics to WandB\n",
    "            log_dict = {\n",
    "                'avg_ep_len': avg_ep_len,\n",
    "                'avg_ep_rew': avg_ep_rew,\n",
    "                'avg_step_time': avg_step_time,\n",
    "            }\n",
    "            wandb.log(log_dict, commit=True, step=actual_steps)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef5e1e7-bc63-4288-9a2e-f165b78a17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandBWriter(KVWriter):\n",
    "    \"\"\"\n",
    "    Log metrics to Weights & Biases when called by .learn()\n",
    "    \n",
    "    More info: https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/logger.html#KVWriter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize run\n",
    "    def __init__(self, run, verbose=1):\n",
    "        super().__init__()\n",
    "        self.run = run\n",
    "        self.verbose = verbose\n",
    "\n",
    "    # Write metrics to W&B project\n",
    "    def write(self, \n",
    "              key_values: Dict[str, Any], \n",
    "              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], \n",
    "              step: int = 0) -> None:\n",
    "        log_dict = {}\n",
    "        \n",
    "        # Go through each key/value pairs\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())):\n",
    "            \n",
    "            if self.verbose >= 2:\n",
    "                print(f\"step={step} | {key} : {value} ({type(value)})\")\n",
    "            \n",
    "            # Skip excluded items\n",
    "            if excluded is not None and \"wandb\" in excluded:\n",
    "                continue\n",
    "                \n",
    "            # Log integers and floats\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    wandb.log(data={key: value}, step=step)\n",
    "                    log_dict[key] = value\n",
    "                \n",
    "        # Print to console\n",
    "        if self.verbose >= 1:\n",
    "            print(f\"Log for steps={step}\")\n",
    "            print(f\"--------------\")\n",
    "            for (key, value) in sorted(log_dict.items()):\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print()\n",
    "                \n",
    "    # Close the W&B run\n",
    "    def close(self) -> None:\n",
    "        self.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdecb53-cf01-41b5-ad9e-224806c4ac7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define train and test function for a single trial\n",
    "\n",
    "A single \"trial\" is fully training and then testing the agent using one set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aebdc5ce-4123-448c-a675-d067932e24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_trial(settings, hparams):\n",
    "    \"\"\"\n",
    "    Training loop used to evaluate a set of hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed\n",
    "    set_random_seeds(settings['seed'])\n",
    "    \n",
    "    # Create new W&B run\n",
    "    config = {}\n",
    "    dt = datetime.datetime.now(datetime.timezone.utc)\n",
    "    dt = dt.replace(microsecond=0, tzinfo=None)\n",
    "    run = wandb.init(\n",
    "        project=settings['wandb_project'], \n",
    "        name=str(dt), \n",
    "        config=config,\n",
    "        settings=wandb.Settings(silent=(not settings['verbose_wandb']))\n",
    "    )\n",
    "\n",
    "    # Print run info\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(f\"WandB run ID: {run.id}\")\n",
    "        print(f\"WandB run name: {run.name}\")\n",
    "    \n",
    "    # Log hyperparameters to W&B\n",
    "    wandb.config.update(hparams)\n",
    "    \n",
    "    # Set custom logger with our custom writer\n",
    "    wandb_writer = WandBWriter(run, verbose=settings['verbose_log'])\n",
    "    loggers = Logger(\n",
    "        folder=None,\n",
    "        output_formats=[wandb_writer]\n",
    "    )\n",
    "    \n",
    "    # Calculate derived hyperparameters\n",
    "    n_steps = 2 ** hparams['steps_per_update_pow2']\n",
    "    minibatch_size = 2 ** hparams['minibatch_size_pow2']\n",
    "    layer_1 = 2 ** hparams['layer_1_pow2']\n",
    "    layer_2 = 2 ** hparams['layer_2_pow2']\n",
    "\n",
    "    # Create new agent\n",
    "    # PPO docs: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "    # Policy networks: https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
    "    model = sb3.PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=hparams['learning_rate'], # Learning rate of neural network (default: 0.0003)\n",
    "        n_steps=n_steps,                        # Number of steps per update (default: 2048)\n",
    "        batch_size=minibatch_size,              # Minibatch size for NN update (default: 64)\n",
    "        gamma=hparams['gamma'],                 # Discount factor (default: 0.99)\n",
    "        gae_lambda=hparams['gae_lambda'],       # Trade-off of bias vs. variance for GAE (default: 0.95)\n",
    "        clip_range=hparams['clip_range'],       # Clipping parameter (default: 0.2)\n",
    "        ent_coef=hparams['entropy_coef'],       # Entropy, how much to explore (default: 0.0)\n",
    "        vf_coef=hparams['vf_coef'],             # Value function coefficient for the loss calculation (default: 0.5)\n",
    "        max_grad_norm=hparams['max_grad_norm'], # Max value for gradient clipping (default: 0.5)\n",
    "        use_sde=hparams['use_sde'],             # Use generalized State Dependent Exploration (default: False)\n",
    "        sde_sample_freq=hparams['sde_freq'],    # Number of steps before sampling new noise matrix (default -1)\n",
    "        policy_kwargs={'net_arch': [layer_1, layer_2]}, # (default: [64, 64])\n",
    "        verbose=settings['verbose_train']       # Print training metrics (default: 0)\n",
    "    )\n",
    "    steps_to_complete = settings['total_steps']\n",
    "        \n",
    "    # Set up checkpoint callback\n",
    "    checkpoint_callback = EvalAndSaveCallback(\n",
    "        check_freq=settings['checkpoint_freq'], \n",
    "        save_dir=settings['save_dir'],\n",
    "        model_name=settings['model_name'],\n",
    "        replay_buffer_name=settings['replay_buffer_name'],\n",
    "        steps_per_test=settings['steps_per_test'],\n",
    "        num_tests=settings['tests_per_check'],\n",
    "        step_offset=(settings['total_steps'] - steps_to_complete),\n",
    "        verbose=settings['verbose_test'],\n",
    "    )\n",
    "    \n",
    "    # Choo choo train\n",
    "    model.learn(total_timesteps=steps_to_complete, \n",
    "                callback=[checkpoint_callback])\n",
    "    \n",
    "    # Get dataframe of run metrics\n",
    "    history = wandb.Api().run(f\"{run.project}/{run.id}\").history()\n",
    "\n",
    "    # Get index of evaluation with maximum reward\n",
    "    max_idx = np.argmax(history.loc[:, 'avg_ep_rew'].values)\n",
    "\n",
    "    # Find number of steps required to produce that maximum reward\n",
    "    max_rew_steps = history['_step'][max_idx]\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(f\"Steps with max reward: {max_rew_steps}\")\n",
    "    \n",
    "    # Load model with maximum reward from previous run\n",
    "    model_path = os.path.join(settings['save_dir'], f\"{settings['model_name']}_{str(max_rew_steps)}.zip\")\n",
    "    model = sb3.PPO.load(model_path, env)\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    avg_ep_len, avg_ep_rew, avg_step_time = evaluate_agent(\n",
    "        env, \n",
    "        model, \n",
    "        settings['steps_per_test'],\n",
    "        settings['tests_per_check'],\n",
    "    )\n",
    "    \n",
    "    # Log final evaluation metrics to WandB run\n",
    "    wandb.run.summary['Average test episode length'] = avg_ep_len\n",
    "    wandb.run.summary['Average test episode reward'] = avg_ep_rew\n",
    "    wandb.run.summary['Average test step time'] = avg_step_time\n",
    "    \n",
    "    # Print final run metrics\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(\"---\")\n",
    "        print(f\"Best model: {settings['model_name']}_{str(max_rew_steps)}.zip\")\n",
    "        print(f\"Average episode length: {avg_ep_len}\")\n",
    "        print(f\"Average episode reward: {avg_ep_rew}\")\n",
    "        print(f\"Average step time: {avg_step_time}\")\n",
    "                      \n",
    "    # Close W&B run\n",
    "    run.finish()\n",
    "    \n",
    "    return avg_ep_rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a7822-6c9e-4fff-a03f-0304d787702d",
   "metadata": {},
   "source": [
    "## Perform trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "288b5de3-822d-4395-b1e8-2537680b0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project settings that do not change\n",
    "settings = {\n",
    "    'wandb_project': \"pendulum-irl-hpo\",\n",
    "    'model_name': \"ppo-pendulum\",\n",
    "    'ax_experiment_name': \"ppo-pendulum-experiment\",\n",
    "    'ax_objective_name': \"avg_ep_rew\",\n",
    "    'replay_buffer_name': None,\n",
    "    'save_dir': \"checkpoints\",\n",
    "    'checkpoint_freq': 10_000,\n",
    "    'steps_per_test': 100,\n",
    "    'tests_per_check': 10,\n",
    "    'total_steps': 100_000,\n",
    "    'num_trials': 100,\n",
    "    'seed': 42,\n",
    "    'verbose_ax': False,\n",
    "    'verbose_wandb': False,\n",
    "    'verbose_train': 0,\n",
    "    'verbose_log': 0,\n",
    "    'verbose_test': 0,\n",
    "    'verbose_trial': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed4bcd36-4d5a-489a-8a4c-86403a6a1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters we want to optimize\n",
    "# Ref: https://github.com/facebook/Ax/blob/6443cee30cbf8cec290200a7420a3db08e4b5445/ax/service/ax_client.py#L236\n",
    "# Example: https://github.com/facebook/Ax/blob/main/tutorials/tune_cnn_service.ipynb\n",
    "# Hyperparameters: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO\n",
    "hparams = [\n",
    "    {\n",
    "        'name': \"n_envs\",\n",
    "        'type': \"fixed\",\n",
    "        'value_type': \"int\",\n",
    "        'value': 1,\n",
    "    },\n",
    "    {\n",
    "        'name': \"learning_rate\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"float\",\n",
    "        'bounds': [1e-5, 1e-2],\n",
    "        'log_scale': True,\n",
    "    },\n",
    "    {\n",
    "        'name': \"steps_per_update_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [6, 12], # Inclusive, 2**n between [64, 4096]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"minibatch_size_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [5, 10], # Inclusive, 2**n between [32, 1024]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"gae_lambda\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"float\",\n",
    "        'bounds': [0.8, 0.99],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"clip_range\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"float\",\n",
    "        'bounds': [0.1, 0.4],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"gamma\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"float\",\n",
    "        'bounds': [0.9, 0.99],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"entropy_coef\",\n",
    "        'value_type': \"float\",\n",
    "        'type': \"range\",\n",
    "        'bounds': [0.0, 0.1],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"vf_coef\",\n",
    "        'value_type': \"float\",\n",
    "        'type': \"range\",\n",
    "        'bounds': [0.15, 0.95],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"max_grad_norm\",\n",
    "        'value_type': \"float\",\n",
    "        'type': \"range\",\n",
    "        'bounds': [0.3, 1.0],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"use_sde\",\n",
    "        'value_type': \"bool\",\n",
    "        'type': \"choice\",\n",
    "        'values': [True, False],\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"sde_freq\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [-1, 8],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"layer_1_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [6, 8], # Inclusive, 2**n between [64, 256]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"layer_2_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [6, 8], # Inclusive, 2**n between [64, 256]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set parameter constraints\n",
    "# Example: https://github.com/facebook/Ax/issues/621\n",
    "parameter_constraints = [\n",
    "    \"minibatch_size_pow2 >= steps_per_update_pow2\" # `batch_size` should be a factor of `n_steps * n_envs`, assume n_envs=1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efc6bb2f-77de-4675-8c1e-6df6c4b3d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af5b3fb4-2186-425e-8079-a54735dde9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "env = Pendulum(\n",
    "        SERIAL_PORT,\n",
    "        BAUD_RATE,\n",
    "        ctrl_timeout=CTRL_TIMEOUT,\n",
    "        debug_level=DEBUG_LEVEL,\n",
    "        env_timeout=ENV_TIMEOUT, \n",
    "        stp_mode=STEP_MODE, \n",
    "        stp_blocking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "362c011c-85ab-4460-a5cd-a2a687feb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosntruct path to Ax experiment snapshot file\n",
    "ax_snapshot_path = os.path.join(settings['save_dir'], f\"{settings['ax_experiment_name']}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8a7eb35-aa2f-47ff-bdbf-de1658632e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER! Uncomment to delete the experiment file to start over\n",
    "\n",
    "# os.remove(ax_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70621040-a54e-4e8b-ba5b-89f30be1730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgmustadio\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\ax\\core\\parameter.py:517: UserWarning:\n",
      "\n",
      "`sort_values` is not specified for `ChoiceParameter` \"use_sde\". Defaulting to `True` for parameters of `ParameterType` BOOL. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "\n",
      "[INFO 09-12 17:25:14] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiment from snapshot: checkpoints\\ppo-pendulum-experiment.json\n"
     ]
    }
   ],
   "source": [
    "# Load experiment from snapshot if it exists, otherwise create a new one\n",
    "# Ref: https://ax.dev/versions/0.2.10/api/service.html#ax.service.ax_client.AxClient.create_experiment\n",
    "if os.path.exists(ax_snapshot_path):\n",
    "    print(f\"Loading experiment from snapshot: {ax_snapshot_path}\")\n",
    "    ax_client = AxClient.load_from_json_file(ax_snapshot_path)\n",
    "else:\n",
    "    print(f\"Creating new experiment. Snapshot to be saved at {ax_snapshot_path}.\")\n",
    "    ax_client = AxClient(\n",
    "        random_seed=settings['seed'],\n",
    "        verbose_logging=settings['verbose_ax'],\n",
    "    )\n",
    "    ax_client.create_experiment(\n",
    "        name=settings['ax_experiment_name'],\n",
    "        parameters=hparams,\n",
    "        objective_name=settings['ax_objective_name'],\n",
    "        minimize=False,\n",
    "        parameter_constraints=parameter_constraints,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e98af995-9535-44d0-b269-16d341f7b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER! Use this cell to mark trials as failed (e.g. if component breaks and WandB shows bad data for a given trial)\n",
    "# Check .json file with a site like https://jsonformatter.org/json-pretty-print\n",
    "\n",
    "# trial_index = 8\n",
    "# trial = ax_client.experiment.trials[trial_index]\n",
    "# trial.mark_failed(unsafe=True)\n",
    "# print(trial)\n",
    "# ax_client.save_to_json_file(ax_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4a184b8-3f2b-48cf-9e07-c8f97dd23ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-12 17:25:17] ax.service.ax_client: Generated new trial 19 with parameters {'learning_rate': 2.1e-05, 'steps_per_update_pow2': 6, 'minibatch_size_pow2': 6, 'gae_lambda': 0.814412, 'clip_range': 0.282224, 'gamma': 0.944903, 'entropy_coef': 0.090967, 'vf_coef': 0.947018, 'max_grad_norm': 0.305853, 'sde_freq': 1, 'layer_1_pow2': 7, 'layer_2_pow2': 7, 'use_sde': True, 'n_envs': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trial 19 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2817918289490c9b172a8081c7856e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666656966, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run ID: by73irtz\n",
      "WandB run name: 2023-09-12 23:25:17\n",
      "Steps with max reward: 80000\n",
      "---\n",
      "Best model: ppo-pendulum_80000.zip\n",
      "Average episode length: 89.1\n",
      "Average episode reward: -597.379748816539\n",
      "Average step time: 0.028475642133491314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-12 18:33:43] ax.service.ax_client: Completed trial 19 with data: {'avg_ep_rew': (-597.379749, None)}.\n",
      "[INFO 09-12 18:33:43] ax.service.ax_client: Saved JSON-serialized state of optimization to `checkpoints\\ppo-pendulum-experiment.json`.\n",
      "[INFO 09-12 18:33:43] ax.service.ax_client: Generated new trial 20 with parameters {'learning_rate': 0.000659, 'steps_per_update_pow2': 10, 'minibatch_size_pow2': 10, 'gae_lambda': 0.974604, 'clip_range': 0.290739, 'gamma': 0.94032, 'entropy_coef': 0.094499, 'vf_coef': 0.624216, 'max_grad_norm': 0.676636, 'sde_freq': 8, 'layer_1_pow2': 7, 'layer_2_pow2': 6, 'use_sde': True, 'n_envs': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trial 20 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run ID: hvcokwp9\n",
      "WandB run name: 2023-09-13 00:33:43\n",
      "Steps with max reward: 100000\n",
      "---\n",
      "Best model: ppo-pendulum_100000.zip\n",
      "Average episode length: 100.0\n",
      "Average episode reward: -291.0181691964932\n",
      "Average step time: 0.04970872807502747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-12 19:50:40] ax.service.ax_client: Completed trial 20 with data: {'avg_ep_rew': (-291.018169, None)}.\n",
      "[INFO 09-12 19:50:40] ax.service.ax_client: Saved JSON-serialized state of optimization to `checkpoints\\ppo-pendulum-experiment.json`.\n",
      "[INFO 09-12 19:50:40] ax.service.ax_client: Generated new trial 21 with parameters {'learning_rate': 4.3e-05, 'steps_per_update_pow2': 6, 'minibatch_size_pow2': 10, 'gae_lambda': 0.924992, 'clip_range': 0.136467, 'gamma': 0.982374, 'entropy_coef': 0.006139, 'vf_coef': 0.789932, 'max_grad_norm': 0.856567, 'sde_freq': 4, 'layer_1_pow2': 6, 'layer_2_pow2': 8, 'use_sde': True, 'n_envs': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trial 21 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd18298542f43798ae316dbd52dd335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run ID: 45ahkh4r\n",
      "WandB run name: 2023-09-13 01:50:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgmustadio\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:148: UserWarning:\n",
      "\n",
      "You have specified a mini-batch size of 1024, but because the `RolloutBuffer` is of size `n_steps * n_envs = 64`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 64\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=64 and n_envs=1)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Perform trial\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m avg_ep_rew \u001b[38;5;241m=\u001b[39m \u001b[43mdo_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_hparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m ax_client\u001b[38;5;241m.\u001b[39mcomplete_trial(\n\u001b[0;32m     16\u001b[0m     trial_index\u001b[38;5;241m=\u001b[39mtrial_index,\n\u001b[0;32m     17\u001b[0m     raw_data\u001b[38;5;241m=\u001b[39mavg_ep_rew,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Save experiment snapshot\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 76\u001b[0m, in \u001b[0;36mdo_trial\u001b[1;34m(settings, hparams)\u001b[0m\n\u001b[0;32m     64\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m EvalAndSaveCallback(\n\u001b[0;32m     65\u001b[0m     check_freq\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_freq\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     66\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     verbose\u001b[38;5;241m=\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose_test\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Choo choo train\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_to_complete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Get dataframe of run metrics\u001b[39;00m\n\u001b[0;32m     80\u001b[0m history \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mApi()\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhistory()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[38], line 162\u001b[0m, in \u001b[0;36mPendulum.reset\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Let the pendulum fall and return to the starting position\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESET_SETTLE_TIME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctrl\u001b[38;5;241m.\u001b[39mstep(CMD_MOVE_TO, [\u001b[38;5;241m0.0\u001b[39m])\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp:\n\u001b[0;32m    165\u001b[0m     \n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# Extract information from controller response\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Choo choo! Perform trials to optimize hyperparameters\n",
    "while True:\n",
    "    \n",
    "    # Get next hyperparameters and end experiment if we've reached max trials\n",
    "    next_hparams, trial_index = ax_client.get_next_trial()\n",
    "    if trial_index >= settings['num_trials']:\n",
    "        break\n",
    "        \n",
    "    # Show that we're starting a new trial\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(f\"--- Trial {trial_index} ---\")\n",
    "        \n",
    "    # Perform trial\n",
    "    avg_ep_rew = do_trial(settings, next_hparams)\n",
    "    ax_client.complete_trial(\n",
    "        trial_index=trial_index,\n",
    "        raw_data=avg_ep_rew,\n",
    "    )\n",
    "    \n",
    "    # Save experiment snapshot\n",
    "    ax_client.save_to_json_file(ax_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea4173-167e-4e42-913e-d8f7f2f17151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
